% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/matrix_extractions.R
\name{matrix_extractions}
\alias{matrix_extractions}
\alias{transition_matrix}
\alias{transition_prob}
\alias{observation_matrix}
\alias{observation_prob}
\alias{reward_matrix}
\alias{reward_val}
\alias{start_vector}
\alias{normalize_POMDP}
\alias{normalize_MDP}
\title{Extract the Transition, Observation or Reward Information from a POMDP}
\usage{
transition_matrix(x, episode = 1, action = NULL, sparse = NULL)

transition_prob(x, action, start.state, end.state, episode = 1)

observation_matrix(x, episode = 1, action = NULL, sparse = NULL)

observation_prob(x, action, end.state, observation, episode = 1)

reward_matrix(x, episode = 1, action = NULL, start.state = NULL, sparse = NULL)

reward_val(x, action, start.state, end.state, observation, episode = 1)

start_vector(x)

normalize_POMDP(x, episode = 1, sparse = NULL)

normalize_MDP(x, episode = 1, sparse = NULL)
}
\arguments{
\item{x}{A \link{POMDP} object.}

\item{episode}{Episode used for time-dependent POMDPs (\link{POMDP}).}

\item{action}{only return the matrix/value for a given action.}

\item{sparse}{logical; return sparse matrices (\link[Matrix:dgCMatrix-class]{Matrix::dgCMatrix}), if the density is below 50\% . \code{NULL} returns the
representation stored in the problem description.}

\item{start.state, end.state}{name of the state.}

\item{observation}{name of observation.}
}
\value{
A list or a list of lists of matrices.
}
\description{
Converts the description of transition probabilities and observation
probabilities in a POMDP into a list of matrices. Individual values or parts of the matrices
can be more efficiently retrieved using the functions ending \verb{_prob} and \verb{_val}.
}
\details{
\code{normalize_POMDP()} returns a new POMDP definition where \code{transition_prob}, \code{observations_prob},
\code{reward}, and \code{start} are normalized to (lists of) matrices and vectors to make access easy.
Also, \code{states}, \code{actions}, and \code{observations} are ordered as given in the problem definition to make safe
access using numerical indices possible.
Normalized POMDP and MDP descriptions are used for C++ based
code (e.g., \code{\link[=simulate_POMDP]{simulate_POMDP()}}) and normalizing them once will save time if the code is called
repeatedly.

\code{start_vector} normalizes the initial belief vector.

See Details section in \link{POMDP} for more details about possible formats for \code{transition_prob}, \code{observations_prob},
\code{reward}, and \code{start}.

\strong{Sparse matrix support}

Sparse matrix representation provided by package \pkg{Matrix} is available for \code{transition_prob}, \code{observations_prob},
\code{reward}. Matrices that have a density above 50\% are kept as a regular dense matrix, matrices with a density
below 50\% are represented as a \link[Matrix:dgCMatrix-class]{Matrix::dgCMatrix}.
}
\examples{
data("Tiger")

# List of |A| transition matrices. One per action in the from states x states
Tiger$transition_prob
transition_matrix(Tiger)
transition_prob(Tiger, action = "listen", start.state = "tiger-left")

# List of |A| observation matrices. One per action in the from states x observations
Tiger$observation_prob
observation_matrix(Tiger)
observation_prob(Tiger, action = "listen", end.state = "tiger-left")

# List of list of reward matrices. 1st level is action and second level is the
#  start state in the form end state x observation
Tiger$reward
reward_matrix(Tiger)
reward_val(Tiger, action = "listen", start.state = "tiger")

# Translate the initial belief vector
Tiger$start
start_vector(Tiger)

# Normalize the whole model
Tiger_norm <- normalize_POMDP(Tiger)
Tiger_norm$transition_prob

## Visualize transition matrix for action 'open-left'
library("igraph")
g <- graph_from_adjacency_matrix(transition_matrix(Tiger)$"open-left", weighted = TRUE)
edge_attr(g, "label") <- edge_attr(g, "weight")

igraph.options("edge.curved" = TRUE)
plot(g, layout = layout_on_grid, main = "Transitions for action 'open=left'")

## Use a function for the Tiger transition model
trans <- function(action, end.state, start.state) {
  ## listen has an identity matrix
  if (action == 'listen')
    if (end.state == start.state) return(1)
    else return(0)

  # other actions have a uniform distribution
  return(1/2)
}

Tiger$transition_prob <- trans
transition_matrix(Tiger)
}
\seealso{
Other POMDP: 
\code{\link{POMDP}()},
\code{\link{plot_belief_space}()},
\code{\link{sample_belief_space}()},
\code{\link{simulate_POMDP}()},
\code{\link{solve_POMDP}()},
\code{\link{solve_SARSOP}()},
\code{\link{update_belief}()},
\code{\link{write_POMDP}()}
}
\author{
Michael Hahsler
}
\concept{POMDP}
