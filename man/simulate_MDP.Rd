% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simulate_MDP.R
\name{simulate_MDP}
\alias{simulate_MDP}
\title{Simulate Trajectories in a MDP}
\usage{
simulate_MDP(
  model,
  n = 100,
  start = NULL,
  horizon = NULL,
  return_states = FALSE,
  epsilon = NULL,
  method = "cpp",
  verbose = FALSE
)
}
\arguments{
\item{model}{a MDP model.}

\item{n}{number of trajectories.}

\item{start}{probability distribution over the states for choosing the
starting states for the trajectories.
Defaults to "uniform".}

\item{horizon}{number of epochs for the simulation. If \code{NULL} then the
horizon for the model is used.}

\item{return_states}{logical; return visited states.}

\item{epsilon}{the probability of random actions  for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.}

\item{method}{\code{'r'} or \code{'cpp'} to perform simulation using an R or faster C++ implementation.}

\item{verbose}{report used parameters.}
}
\value{
A list with elements:
\itemize{
\item \code{avg_reward}: The average discounted reward.
\item \code{reward}: Reward for each trajectory.
\item \code{action_cnt}: Action counts.
\item \code{state_cnt}: State counts.
\item \code{states}: a vector with state ids.
Rows represent trajectories.
}

A vector with state ids (in the final epoch or all). Attributes containing action
counts, and rewards  for each trajectory may be available.
}
\description{
Simulate trajectories through a MDP. The start state for each
trajectory is randomly chosen using the specified belief. The belief is used to choose actions
from an epsilon-greedy policy and then update the state.
}
\examples{
data(Maze)

# solve the POMDP for 5 epochs and no discounting
sol <- solve_MDP(Maze, discount = 1)
sol
policy(sol)
# U in the policy is and estimate of the utility of being in a state when using the optimal policy.

## Example 1: simulate 10 trajectories, only the final belief state is returned
sim <- simulate_MDP(sol, n = 100, horizon = 10, verbose = TRUE)
sim

# Calculate proportion of actions used
sim$action_cnt / sum(sim$action_cnt)

# reward distribution
hist(sim$reward)

## Example 2: simulate starting always in state s_1 and return all visited states
sim <- simulate_MDP(sol, n = 100, start = "s_1", horizon = 10, return_states = TRUE)
sim$avg_reward

# how often was each state visited?
table(sim$states)
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{solve_MDP}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
