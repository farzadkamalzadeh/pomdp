% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{random_policy}
\title{Solve an MDP Problem}
\usage{
solve_MDP(
  model,
  horizon = NULL,
  discount = NULL,
  terminal_values = NULL,
  method = "value",
  eps = 0.01,
  max_iterations = 1000,
  k_backups = 10,
  verbose = FALSE
)

random_policy(model, prob = NULL)
}
\arguments{
\item{model}{a POMDP problem specification created with \code{\link[=POMDP]{POMDP()}}.
Alternatively, a POMDP file or the URL for a POMDP file can be specified.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.  For
time-dependent POMDPs a vector of horizons can be specified (see Details
section).}

\item{discount}{discount factor in range \eqn{[0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{terminal_values}{a vector with terminal utilities for each state. If
\code{NULL}, then a vector of all 0s is used.}

\item{method}{string; one of the following solution methods: \code{'value'},
\code{'policy'}.}

\item{eps}{maximum error allowed in the utility of any state
(i.e., the maximum policy loss).}

\item{max_iterations}{maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.}

\item{k_backups}{number of look ahead steps used for approximate policy evaluation
used by method \code{'policy'}.}

\item{verbose}{logical, if set to \code{TRUE}, the function provides the
output of the pomdp solver in the R console.}

\item{prob}{probability vector for actions.}
}
\value{
The solver returns an object of class POMDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
}
\description{
A simple implementation of value iteration and modified policy iteration.
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value")
policy(maze_solved)

plot_value_function(maze_solved)

# use modified policy iteration
maze_solved <- solve_MDP(Maze, method = "policy")
policy(maze_solved)

# finite horizon
maze_solved <- solve_MDP(Maze, method = "value", horizon = 3)
policy(maze_solved)

}
\author{
Michael Hahsler
}
