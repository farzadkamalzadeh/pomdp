% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{solve_MDP_DP}
\alias{solve_MDP_TD}
\title{Solve an MDP Problem}
\usage{
solve_MDP(model, method = "value", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  terminal_values = NULL,
  eps = 0.01,
  max_iterations = 1000,
  k_backups = 10,
  verbose = FALSE
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = 0.1,
  epsilon = 0.1,
  N = 100,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'value_iteration'},
\code{'policy_iteration'}, \code{'q_learning'}, \code{'sarsa'}, or \code{'expected_sarsa'}.}

\item{...}{further parameters are passed on to the solver function.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{terminal_values}{a vector with terminal utilities for each state. If
\code{NULL}, then a vector of all 0s is used.}

\item{eps}{maximum error allowed in the utility of any state
(i.e., the maximum policy loss) used as the termination criterion for
the value iteration method.}

\item{max_iterations}{maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.}

\item{k_backups}{number of look ahead steps used for approximate policy evaluation
used by the policy iteration method.}

\item{verbose}{logical, if set to \code{TRUE}, the function provides the
output of the solver in the R console.}

\item{alpha}{step size in \verb{(0, 1]}.}

\item{epsilon}{used for \eqn{\epsilon}-greedy policies.}

\item{N}{number of episodes used for learning.}
}
\value{
\code{solve_MDP()} returns an object of class POMDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final delta (infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
}
\details{
Implemented are the following dynamic programming methods:
\itemize{
\item \strong{Modified Policy Iteration}
starts with a random policy and iteratively performs
a sequence of
\itemize{
\item approximate policy evaluation (estimate the value function for the
current policy using \code{k_backups}), and
\item policy improvement (calculate a greedy policy given the value function).
The algorithm stops when it converges to a stable policy.
}
\item \strong{Value Iteration} starts with
an arbitraty value function (we use all 0s) and iteratively
updates the value function using the Bellman equation. The iterations
are terminated when no state value in the value function changes by more than
\eqn{\epsilon (1-\gamma) / \gamma} which means that no state value is more than
\eqn{\epsilon} from the value in the optimal value function. The greedy policy
is calculated from the value function. Value iteration can be seen as
policy iteration with truncated policy evaluation.
}

Implemented are the following temporal difference control methods. Note
that the MDP transition and reward models are only used to simulate
the environment for these reinforcement learning methods.
The algorithms use a step size parameter \eqn{\alpha} (learning rate) for the
updates and the exploration parameter \eqn{\epsilon} for
the \eqn{\epsilon}-greedy policy.
\itemize{
\item \strong{Q-Learning} is an off-policy temporal difference method that uses
an \eqn{\epsilon}-greedy behavior policy and learns a greedy target
policy.
\item \strong{Sarsa} is an on-policy method that learns an \eqn{\epsilon}-greedy
policy.
\item \strong{Expected Sarsa}: We implement an on-policy version that uses
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa
moved in expectation. Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and even 1.
}
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U (with the state labels rotated)
plot_value_function(maze_solved)

# Maze solutions can be visualized
plot_Maze_solution(maze_solved)

# use modified policy iteration
maze_solved <- solve_MDP(Maze, method = "policy_iteration")
policy(maze_solved)

# finite horizon
maze_solved <- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
plot_Maze_solution(maze_solved, epoch = 1)
plot_Maze_solution(maze_solved, epoch = 2)
plot_Maze_solution(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted <- Maze
Maze_discounted$discount <- .9
pi <- random_MDP_policy(Maze_discounted, 
        prob = c(n = .7, e = .1, s = .1, w = 0.1))
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved <- solve_MDP(Maze)

approx_MDP_policy_evaluation(pi, Maze, k_backup = 100)
approx_MDP_policy_evaluation(policy(maze_solved)[[1]], Maze, k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "q_learning", N = 100)
maze_learned

maze_learned$solution
policy(maze_learned)
plot_value_function(maze_learned)
plot_Maze_solution(maze_learned)
}
\seealso{
Other solver: 
\code{\link{solve_POMDP}()},
\code{\link{solve_SARSOP}()}

Other MDP: 
\code{\link{MDP_policy_functions}},
\code{\link{MDP}()},
\code{\link{POMDP_accessors}},
\code{\link{simulate_MDP}()},
\code{\link{transition_graph}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{solver}
