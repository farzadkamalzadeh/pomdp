% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MDP_policy_funcitons.R
\name{MDP_policy_functions}
\alias{MDP_policy_functions}
\alias{q_values_MDP}
\alias{approx_MDP_policy_evaluation}
\alias{greedy_MDP_action}
\alias{greedy_MDP_policy}
\alias{random_MDP_policy}
\title{Functions for MDP Policies}
\usage{
q_values_MDP(model, U = NULL)

approx_MDP_policy_evaluation(pi, model, U = NULL, k_backups = 10)

greedy_MDP_action(s, Q, epsilon = 0, prob = FALSE)

greedy_MDP_policy(Q)

random_MDP_policy(model, prob = NULL)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{U}{a vector with value function representing the state utilities
(expected sum of discounted rewards from that point on).
If \code{model} is a solved model, then the state
utilities are taken from the solution.}

\item{pi}{a policy as a data.frame with at least columns for states and action.}

\item{k_backups}{number of look ahead steps used for approximate policy evaluation
used by the policy iteration method.}

\item{s}{a state.}

\item{Q}{an action value function with Q-values as a state by action matrix.}

\item{epsilon}{an \code{epsilon > 0} creates an epsilon-greedy policies.}

\item{prob}{probability vector for random actions for random_MDP_policy().
a logical indicating if action probabilities should be returned for
\code{greedy_MDP_action()}.}
}
\value{
\code{q_values_MDP()} returns a state by action matrix specifying the Q-function,
i.e., the action value for executing each action in each state. The Q-values
are calculated from the value function (U) and the transition model.

\code{approx_MDP_policy_evaluation()} returns a vector with approximate
state values (U).

\code{greedy_MDP_action()} returns the action with the highest q-value
for state \code{s}. If \code{prob = TRUE}, then a vector with
the probability for each action is returned.

\code{greedy_MDP_policy()} returns the greedy policy given \code{Q}.

\code{random_MDP_policy()} returns a data.frame with the columns state and action to define a policy.
}
\description{
Implementation several functions useful to deal with MDP policies.
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
maze_solved

pi_opt <- policy(maze_solved)

# calculate the Q-function (action-value function)
q <- q_values_MDP(maze_solved)
q

# create a random policy for 
pi_random <- random_MDP_policy(Maze)
pi_random

# improve the random policy by one policy evaluation and
#   policy improvement step
u <- approx_MDP_policy_evaluation(pi_random, Maze)
q <- q_values_MDP(Maze, U = u)
pi_greedy <- greedy_MDP_policy(q)

# estimate the the value function for policies
rbind(
  random = approx_MDP_policy_evaluation(pi_random, Maze),
  greedy = approx_MDP_policy_evaluation(pi_greedy, Maze),
  opt = approx_MDP_policy_evaluation(pi_opt, Maze)
)

# calculate greedy action for state 1 
greedy_MDP_action(1, q, epsilon = 0, prob = FALSE)
greedy_MDP_action(1, q, epsilon = 0, prob = TRUE)
greedy_MDP_action(1, q, epsilon = .1, prob = TRUE)

# add the random policy to the maze to create a "solved" MDP
maze_random <- add_policy(Maze, pi_greedy)
maze_random

plot_value_function(maze_random)
plot_Maze_solution(maze_random)
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{POMDP_accessors}},
\code{\link{simulate_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{value_function}()}
}
\concept{MDP}
